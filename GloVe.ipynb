{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GloVe embeddings of word meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from toxic_funcs import Cleaner\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading only first n for testing purposes\n",
    "n = 10000\n",
    "df = pd.read_csv(\"data/train.csv\").set_index('id')\n",
    "df = df.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running through the custom cleaner\n",
    "from re import subn\n",
    "\n",
    "clean_comments = []\n",
    "for comment in df.comment_text.values:\n",
    "    cleaned, _ = subn('[(){[]}:;.,]', '', Cleaner(comment))\n",
    "    clean_comments.append(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slapping it onto df\n",
    "\n",
    "df['cleaned'] = clean_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "texts = df.cleaned.values\n",
    "labels_index = {0: 0, 1:1}\n",
    "labels = df.toxic.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\nstein\\appdata\\local\\conda\\conda\\envs\\pydata\\lib\\site-packages\\keras\\preprocessing\\text.py:157: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36625 unique tokens.\n",
      "Shape of data tensor: (10000, 1000)\n",
      "Shape of label tensor: (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "VALIDATION_SPLIT =.3\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "X_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "X_test = data[-nb_validation_samples:]\n",
    "y_test = labels[-nb_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "GLOVE_DIR = os.path.join('..','TEXT_DATA_DIR', 'glove.6B')\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM  = 100\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with GloVe\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"flatten_3/Reshape:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(256, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "print(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.2)(x) \n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(12, activation='relu')(x)\n",
    "\n",
    "\n",
    "\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "7000/7000 [==============================] - 99s 14ms/step - loss: 0.3909 - acc: 0.9054 - val_loss: 0.3350 - val_acc: 0.8957\n",
      "Epoch 2/10\n",
      "7000/7000 [==============================] - 106s 15ms/step - loss: 0.3208 - acc: 0.9060 - val_loss: 0.3341 - val_acc: 0.8957\n",
      "Epoch 3/10\n",
      "7000/7000 [==============================] - 106s 15ms/step - loss: 0.3169 - acc: 0.9059 - val_loss: 0.3330 - val_acc: 0.8957\n",
      "Epoch 4/10\n",
      "7000/7000 [==============================] - 95s 14ms/step - loss: 0.3129 - acc: 0.9064 - val_loss: 0.3277 - val_acc: 0.8963\n",
      "Epoch 5/10\n",
      "7000/7000 [==============================] - 94s 13ms/step - loss: 0.3104 - acc: 0.9069 - val_loss: 0.3716 - val_acc: 0.8963\n",
      "Epoch 6/10\n",
      "7000/7000 [==============================] - 94s 13ms/step - loss: 0.3037 - acc: 0.9073 - val_loss: 0.3522 - val_acc: 0.8963\n",
      "Epoch 7/10\n",
      "7000/7000 [==============================] - 95s 14ms/step - loss: 0.3036 - acc: 0.9070 - val_loss: 0.3472 - val_acc: 0.8960\n",
      "Epoch 8/10\n",
      "7000/7000 [==============================] - 94s 13ms/step - loss: 0.3003 - acc: 0.9071 - val_loss: 0.3817 - val_acc: 0.8863\n",
      "Epoch 9/10\n",
      "7000/7000 [==============================] - 94s 13ms/step - loss: 0.2973 - acc: 0.9084 - val_loss: 0.3412 - val_acc: 0.8963\n",
      "Epoch 10/10\n",
      "7000/7000 [==============================] - 94s 13ms/step - loss: 0.2960 - acc: 0.9094 - val_loss: 0.3434 - val_acc: 0.8973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x282a1240>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# happy learning!\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "          epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network Test - Loss: 0.7007125447591146, Accuracy: 0.35399999992052716\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Normal Neural Network Test - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM RNN\n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         3662600   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1000, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 3,753,302\n",
      "Trainable params: 90,702\n",
      "Non-trainable params: 3,662,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "7000/7000 [==============================] - 116s 17ms/step - loss: 0.2773 - acc: 0.9028\n",
      "Epoch 2/10\n",
      "7000/7000 [==============================] - 124s 18ms/step - loss: 0.1798 - acc: 0.9345\n",
      "Epoch 3/10\n",
      "7000/7000 [==============================] - 116s 17ms/step - loss: 0.1721 - acc: 0.9370\n",
      "Epoch 4/10\n",
      "7000/7000 [==============================] - 121s 17ms/step - loss: 0.1449 - acc: 0.9454\n",
      "Epoch 5/10\n",
      "7000/7000 [==============================] - 119s 17ms/step - loss: 0.1383 - acc: 0.9501\n",
      "Epoch 6/10\n",
      "7000/7000 [==============================] - 115s 16ms/step - loss: 0.1344 - acc: 0.9493\n",
      "Epoch 7/10\n",
      "7000/7000 [==============================] - 115s 16ms/step - loss: 0.1241 - acc: 0.9521\n",
      "Epoch 8/10\n",
      "7000/7000 [==============================] - 124s 18ms/step - loss: 0.1215 - acc: 0.9537\n",
      "Epoch 9/10\n",
      "7000/7000 [==============================] - 119s 17ms/step - loss: 0.1135 - acc: 0.9579\n",
      "Epoch 10/10\n",
      "7000/7000 [==============================] - 118s 17ms/step - loss: 0.1133 - acc: 0.9589\n",
      "Accuracy: 94.35%\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Normal Neural Network Test - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vanilla bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Neural Network model here\n",
    "# YOUR CODE HERE\n",
    "model = Sequential()\n",
    "model.add(Dense(1000,activation = \"relu\", input_dim = 10000))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(100, activation = \"relu\"))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(2, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7500/7500 [==============================] - 34s 5ms/step - loss: 0.2603 - acc: 0.9145\n",
      "Epoch 2/10\n",
      "7500/7500 [==============================] - 34s 4ms/step - loss: 0.0982 - acc: 0.9664\n",
      "Epoch 3/10\n",
      "7500/7500 [==============================] - 35s 5ms/step - loss: 0.0261 - acc: 0.9943\n",
      "Epoch 4/10\n",
      "7500/7500 [==============================] - 35s 5ms/step - loss: 0.0152 - acc: 0.9987\n",
      "Epoch 5/10\n",
      "7500/7500 [==============================] - 35s 5ms/step - loss: 0.0118 - acc: 0.9991\n",
      "Epoch 6/10\n",
      "7500/7500 [==============================] - 35s 5ms/step - loss: 0.0118 - acc: 0.9991\n",
      "Epoch 7/10\n",
      "7500/7500 [==============================] - 35s 5ms/step - loss: 0.0110 - acc: 0.9993\n",
      "Epoch 8/10\n",
      "7500/7500 [==============================] - 34s 5ms/step - loss: 0.0111 - acc: 0.9992\n",
      "Epoch 9/10\n",
      "7500/7500 [==============================] - 36s 5ms/step - loss: 0.0109 - acc: 0.9993\n",
      "Epoch 10/10\n",
      "7500/7500 [==============================] - 34s 5ms/step - loss: 0.0116 - acc: 0.9991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5963acc0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "X = tokenizer.texts_to_matrix(df.cleaned.values)\n",
    "y = to_categorical(df.toxic.values)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, epochs = 10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network Test - Loss: 0.45488132861237973, Accuracy: 0.9496\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Normal Neural Network Test - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
